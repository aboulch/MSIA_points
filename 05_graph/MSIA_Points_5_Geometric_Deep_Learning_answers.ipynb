{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18nR8zB1ObJY"
      },
      "source": [
        "# Point cloud classification\n",
        "\n",
        "In this practical session, we will train our first neural networks for point processing.\n",
        "The task we target in point cloud classification. Analagous to image classification, the objective is given a point cloud $P$ to predict the class $c$.\n",
        "\n",
        "The notebook follows a classic setup for classification pipeline.\n",
        "1. notebook setup and data preparation\n",
        "2. data augmentation creation\n",
        "3. metrics\n",
        "4. training and validation loop\n",
        "5. network definition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook and data setup\n",
        "\n",
        "We first import the librairies needed for the practical session."
      ],
      "metadata": {
        "id": "wlfVyOMZVcsM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQMAGBi7OK7q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from huggingface_hub import hf_hub_download\n",
        "import plotly.graph_objects as go # for visualization\n",
        "import tqdm\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.spatial import KDTree\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we download the [Modelnet40 dataset](https://modelnet.cs.princeton.edu/), which contains 9840 (resp. 2468) shapes for training (resp. validation). The shapes are divided into 40 categories."
      ],
      "metadata": {
        "id": "P7LcKlZhsbg_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9-tZygLUeZg"
      },
      "outputs": [],
      "source": [
        "hf_hub_download(repo_id=\"Msun/modelnet40\", filename=\"modelnet40_ply_hdf5_2048.zip\", repo_type=\"dataset\", cache_dir=\".\")\n",
        "!unzip ./datasets--Msun--modelnet40/snapshots/d5dc795541800feeb7a4b3bd3142729a0d2adf7a/modelnet40_ply_hdf5_2048"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As now usual, we also define the function to visualize the point clouds using Plotly."
      ],
      "metadata": {
        "id": "BkFXzP7kuuvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHTJ-_6NYgX2"
      },
      "outputs": [],
      "source": [
        "# display the point cloud\n",
        "def point_cloud_visu(pts, cls=None):\n",
        "\n",
        "    fig = go.Figure(\n",
        "        data=[\n",
        "            go.Scatter3d(\n",
        "                x=pts[:,0], y=pts[:,1], z=pts[:,2],\n",
        "                mode='markers',\n",
        "                marker=dict(size=3,\n",
        "                            color=cls,\n",
        "                            colorscale='Viridis',\n",
        "                            )\n",
        "            )\n",
        "        ],\n",
        "        layout=dict(\n",
        "            scene=dict(\n",
        "                xaxis=dict(visible=False),\n",
        "                yaxis=dict(visible=False),\n",
        "                zaxis=dict(visible=False),\n",
        "                aspectmode=\"data\", #this string can be 'data', 'cube', 'auto', 'manual'\n",
        "                #a custom aspectratio is defined as follows:\n",
        "                aspectratio=dict(x=1, y=1, z=0.95)\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the last function of the notebook setup concerns data loading.\n",
        "The files are in hdf5 format. They come is in parts, are loaded separately and concatenated."
      ],
      "metadata": {
        "id": "Mmnqz6Fdu5KR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvFohQW-Xw1P"
      },
      "outputs": [],
      "source": [
        "def get_data(rootdir, files):\n",
        "\n",
        "  filenames = []\n",
        "  for line in open(os.path.join(rootdir, files)):\n",
        "      line = line.split(\"\\n\")[0]\n",
        "      line = os.path.basename(line)\n",
        "      filenames.append(os.path.join(rootdir, line))\n",
        "\n",
        "  data = []\n",
        "  labels = []\n",
        "  for filename in filenames:\n",
        "      f = h5py.File(filename, \"r\")\n",
        "      data.append(f[\"data\"])\n",
        "      labels.append(f[\"label\"])\n",
        "\n",
        "  data = np.concatenate(data, axis=0)\n",
        "  labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "  data = data[:,:,[0,2,1]] # for convenience we put the axis in the usual order\n",
        "\n",
        "  return data, labels.ravel()\n",
        "\n",
        "\n",
        "data, labels = get_data(\"modelnet40_ply_hdf5_2048\", \"train_files.txt\")\n",
        "data = torch.tensor(data, dtype=torch.float)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "print(f\"Data (points): {data.shape} - Labels: {labels.shape}, num_labels {labels.max()+1}\")\n",
        "point_cloud_visu(data[0], data[0,:,2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data tranforms\n",
        "\n",
        "We will first define the trasnformations to be applied to the point cloud.\n",
        "\n",
        "#### Random decimation\n",
        "In order to use smaller point clouds (easier with cpu notebooks), we will first implement random decimation of the point clouds.\n",
        "The class randomly select `num_points` from the original poirn cloud.\n",
        "\n",
        "The `__call__` method of the class takes as input a dictionary (`data_dict`) with two entries, \"points\" and \"labels\", and returns the same dictionary with updated filed \"points\".\n",
        "\n",
        "**Question:** implement the `__call__` method of the class."
      ],
      "metadata": {
        "id": "sNH1JT7oVUbZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2AVKq_AVtu9"
      },
      "outputs": [],
      "source": [
        "class RandomDecimation:\n",
        "  def __init__(self, num_points) -> None:\n",
        "     self.n_pts = num_points\n",
        "\n",
        "  def __call__(self, data_dict):\n",
        "    # fill here\n",
        "    points = data_dict[\"points\"]\n",
        "    ids = torch.randperm(points.shape[0],dtype=torch.long)[:self.n_pts]\n",
        "    data_dict[\"points\"] = points[ids]\n",
        "    return data_dict\n",
        "\n",
        "points = data[0].clone()\n",
        "label = labels[0]\n",
        "data_dict = {\"points\":points, \"labels\":label}\n",
        "transform = RandomDecimation(512)\n",
        "points_t1 = transform(data_dict)[\"points\"] + torch.tensor([2.,0,0])\n",
        "points = torch.cat([points, points_t1], dim=0)\n",
        "point_cloud_visu(points, points[:,2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random rotation\n",
        "\n",
        "A classic augmentation when training with point clouds is to apply random rotation to the point clouds.\n",
        "\n",
        "The class `RandomRotationZ` operates random rotation around the $z$-axis.\n",
        "\n",
        "$$P_\\text{rot} = P * M$$\n",
        "\n",
        "where $M$ is the rotation matrix.\n",
        "\n",
        "**Question:** implement the `__call__` method of the class."
      ],
      "metadata": {
        "id": "eAuTOAhb0sQr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpWs5pSgqVDo"
      },
      "outputs": [],
      "source": [
        "class RandomRotationZ:\n",
        "  def __call__(self, data_dict):\n",
        "    # fill here\n",
        "    points = data_dict[\"points\"]\n",
        "    theta = (torch.rand((1,)) * torch.pi * 2).item()\n",
        "    rot = torch.tensor(\n",
        "        [[ np.cos(theta), np.sin(theta), 0],\n",
        "         [-np.sin(theta), np.cos(theta), 0],\n",
        "         [             0,             0, 1]], dtype=torch.float)\n",
        "    data_dict[\"points\"] = points@rot\n",
        "    return data_dict\n",
        "\n",
        "points = data[0].clone()\n",
        "label = labels[0]\n",
        "data_dict = {\"points\":points, \"labels\":label}\n",
        "transform = RandomRotationZ()\n",
        "points_t1 = transform(data_dict)[\"points\"] + torch.tensor([2,0,0])\n",
        "points_t2 = transform(data_dict)[\"points\"] + torch.tensor([4,0,0])\n",
        "points = torch.cat([points, points_t1, points_t2], dim=0)\n",
        "point_cloud_visu(points, points[:,2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random scaling\n",
        "\n",
        "As a second augmentation, we implement a random scaling which modify the scale of the point cloud randomly in a range $[\\text{scale}_\\text{min}, \\text{scale}_\\text{max}]$.\n",
        "$$P_\\text{scaled} = \\text{scale}*P$$\n",
        "\n",
        "**Question** create the class RandomScale, which as before as a `__init__` for intialization of the scale range and a `__call__` to operate on the data dictionary."
      ],
      "metadata": {
        "id": "rI6ddmFQ4_zR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im1xS0ZgZgsZ"
      },
      "outputs": [],
      "source": [
        "class RandomScale:\n",
        "    def __init__(self, scale_min, scale_max):\n",
        "      self.mini = scale_min\n",
        "      self.maxi = scale_max\n",
        "\n",
        "    def __call__(self, data_dict):\n",
        "      points = data_dict[\"points\"]\n",
        "      scale_factor = torch.rand((1,)).item() * (self.maxi - self.mini) + self.mini\n",
        "      data_dict[\"points\"] = points * scale_factor\n",
        "      return data_dict\n",
        "\n",
        "points = data[0].clone()\n",
        "label = labels[0]\n",
        "data_dict = {\"points\":points, \"labels\":label}\n",
        "transform = RandomScale(scale_min=0.1, scale_max=2.)\n",
        "points_t1 = transform(data_dict)[\"points\"] + torch.tensor([2.,0,0])\n",
        "points_t2 = transform(data_dict)[\"points\"] + torch.tensor([4.,0,0])\n",
        "points = torch.cat([points, points_t1, points_t2], dim=0)\n",
        "point_cloud_visu(points, points[:,2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Composition\n",
        "\n",
        "We need them to assemble the transforms that we defined into an a transformation pipeline.\n",
        "To do so, we create a class `Compose` (similar in effect to the torchvision one), this class takes as contructor argument a list of transform, and the `__call__` method takes a data dictionary and iterate over the transforms and outputs the updated dictionary.\n",
        "\n",
        "**Question:** create the class `Compose`"
      ],
      "metadata": {
        "id": "-R3jCN16y73c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Compose:\n",
        "    def __init__(self, transform_list):\n",
        "        self.t = transform_list\n",
        "\n",
        "    def __call__(self, data_dict):\n",
        "        for t in self.t:\n",
        "            data_dict = t(data_dict)\n",
        "        return data_dict\n",
        "\n",
        "transform = Compose(\n",
        "    [\n",
        "        RandomDecimation(1024),\n",
        "        RandomRotationZ(),\n",
        "        RandomScale(0.9,1.1),\n",
        "    ])\n",
        "\n",
        "points = data[0].clone()\n",
        "label = labels[0]\n",
        "data_dict = {\"points\":points, \"labels\":label}\n",
        "points_t1 = transform(data_dict)[\"points\"] + torch.tensor([2.,0,0])\n",
        "points_t2 = transform(data_dict)[\"points\"] + torch.tensor([4.,0,0])\n",
        "points = torch.cat([points, points_t1, points_t2], dim=0)\n",
        "point_cloud_visu(points, points[:,2])"
      ],
      "metadata": {
        "id": "XuIncHmZs20D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelnet40 dataset\n",
        "\n",
        "Now we create the `Modelnet40Dataset`, the class that will be used to load the data. It inherits from the default torch `Dataset` class.\n",
        "It implements three methods:\n",
        "* `__init__` which takes as argument the root dorectory of the data, the split file, and possible transformations. It loads the data;\n",
        "* `__len__` returns the number of samples in the split\n",
        "* `forward` takes as input an id and return the corresponding data dictionary (after going through the transforms)\n",
        "\n",
        "**Question:** implement the `Modelnet40Dataset` class."
      ],
      "metadata": {
        "id": "6Qo7-jy51Q_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLorCRimYpZz"
      },
      "outputs": [],
      "source": [
        "# create the dataloader\n",
        "class Modelnet40Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, rootdir, files, transforms=None) -> None:\n",
        "        super().__init__()\n",
        "        self.rootdir = rootdir\n",
        "        self.files = files\n",
        "        self.transforms = transforms\n",
        "        self.data, self.labels = get_data(rootdir, files)\n",
        "        self.data = torch.tensor(self.data, dtype=torch.float)\n",
        "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      points = self.data[idx]\n",
        "      label = self.labels[idx]\n",
        "      data_dict = {\"points\": points, \"labels\": label}\n",
        "\n",
        "      if self.transforms is not None:\n",
        "        data_dict = self.transforms(data_dict)\n",
        "\n",
        "      return data_dict\n",
        "\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        RandomDecimation(1024),\n",
        "        RandomRotationZ(),\n",
        "        RandomScale(0.9,1.1),\n",
        "    ])\n",
        "\n",
        "train_dataset = Modelnet40Dataset(rootdir=\"modelnet40_ply_hdf5_2048\", files=\"train_files.txt\", transforms=train_transforms)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=1)\n",
        "\n",
        "for data in train_dataloader:\n",
        "  print(data[\"points\"].shape, data[\"labels\"].shape)\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics\n",
        "\n",
        "We need to quantitatively evaluate our model.\n",
        "Todo so, we will implement accuracy metrics, computed based on the confusion matrix.\n",
        "\n",
        "**Question:** fill the function that compute the confusion matrix. It takes as argument the predictions (array of integers), a batch dictionnary (labels are in \"labels\") and the number of classes."
      ],
      "metadata": {
        "id": "c1sUCtb0VHF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6fpYn93gIFw"
      },
      "outputs": [],
      "source": [
        "def conf_matrix(predictions, batch, num_classes):\n",
        "  pred_labels = torch.argmax(predictions, dim=-1).cpu().numpy()\n",
        "  labels = batch[\"labels\"].cpu().numpy()\n",
        "  cm = confusion_matrix(labels, pred_labels, labels=torch.arange(num_classes))\n",
        "  return cm\n",
        "\n",
        "predictions = torch.rand((100,6))\n",
        "labels = torch.randint(0,6,(100,))\n",
        "batch = {\"labels\": labels}\n",
        "cm = conf_matrix(predictions, batch, 6)\n",
        "plt.imshow(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** fill the `global_accuracy` function that compute the accuracy given a confusion matrix. That is:\n",
        "\n",
        "$$A = \\frac{\\text{TP}}{|P|}$$\n",
        "where $\\text{TP}$ is the number of true positive (overall)"
      ],
      "metadata": {
        "id": "S95uuoO-6NTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def global_accuracy(cm):\n",
        "  return np.diag(cm).sum() / cm.sum()\n",
        "\n",
        "print(global_accuracy(cm))"
      ],
      "metadata": {
        "id": "9iUzH4L16dMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** fill the `accuracy_per_class` function, that computes the accuracy per class (returns an array of accuracies)."
      ],
      "metadata": {
        "id": "FqpTG7O76hJX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dC5CQY4gKFD"
      },
      "outputs": [],
      "source": [
        "def accuracy_per_class(cm):\n",
        "  tp = np.diag(cm)\n",
        "  tp_fn = cm.sum(axis=1)\n",
        "  tp_fn[tp_fn==0] = 1\n",
        "  return tp / tp_fn\n",
        "\n",
        "print(accuracy_per_class(cm))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we implement a function that aggregate the metrics.\n",
        "\n",
        "**Question:** fill the `get_metrics` function. Argument is the confusion matrix, and the output is a dictionary containing:\n",
        "* `confusion_matrix`: the confusion matrix\n",
        "* `accuracy_per_class`: the accuracy per class (array)\n",
        "* `average_accuracy`: the average accuracy per class\n",
        "* `accuracy`: the global accuracy"
      ],
      "metadata": {
        "id": "kUNWD5yk7lNs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-vu4fuwgPk1"
      },
      "outputs": [],
      "source": [
        "def get_metrics(cm):\n",
        "  acc = global_accuracy(cm)\n",
        "  acc_class = accuracy_per_class(cm)\n",
        "  macc = acc_class.mean()\n",
        "\n",
        "  return {\"accuracy\": acc,\n",
        "          \"accuracy_per_class\": acc_class,\n",
        "          \"average_accuracy\": macc,\n",
        "          \"confusion_matrix\": cm\n",
        "          }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and validation loops\n",
        "\n",
        "We now enter the core of the training functions.\n",
        "\n",
        "**Question:** implement the function `classif_loss`, that compute the cross entropy and a batch data (labels are in \"labels\") and returns the loss."
      ],
      "metadata": {
        "id": "3bXTHXrgU_Ta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODpSaEUggGc7"
      },
      "outputs": [],
      "source": [
        "def classif_loss(predictions, batch):\n",
        "  labels = batch[\"labels\"]\n",
        "  loss = torch.nn.functional.cross_entropy(predictions, labels)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** fill the function `create_optimizer` that create an AdamW optimizer for the network passed as argument. It returns the optimizer."
      ],
      "metadata": {
        "id": "ExaCOMTZ9iRL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BKzw7cLgVue"
      },
      "outputs": [],
      "source": [
        "def create_optimizer(network):\n",
        "  optimizer = torch.optim.AdamW(network.parameters(), lr=1e-3)\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We give a `to_device` function that takes a batch and convert it to a tensor on a given device."
      ],
      "metadata": {
        "id": "2oc4NihB-MqV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF5ndcozgXqs"
      },
      "outputs": [],
      "source": [
        "def to_device(batch, device):\n",
        "  if isinstance(batch, torch.Tensor):\n",
        "    return batch.to(device)\n",
        "  elif isinstance(batch, list):\n",
        "    batch_ = []\n",
        "    for elem in batch:\n",
        "      if isinstance(elem, torch.Tensor):\n",
        "        elem = elem.to(device)\n",
        "      batch_.append(elem)\n",
        "    return batch_\n",
        "  elif isinstance(batch, dict):\n",
        "    batch_ = {}\n",
        "    for key, elem in batch.items():\n",
        "      if isinstance(elem, torch.Tensor):\n",
        "        elem = elem.to(device)\n",
        "      batch_[key] = elem\n",
        "    return batch_\n",
        "  else:\n",
        "    raise ValueError(\"unknow batch type\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now implement the training loop per se.\n",
        "The training loop takes as argument:\n",
        "* a network\n",
        "* a train dataloader\n",
        "* an optimizer\n",
        "* a device\n",
        "It does several things:\n",
        "- it performs an optimization loop over the training dataloader;\n",
        "- it computes the overall confusion matrix (iteratively)\n",
        "- it prints the accuracy and average accuracy computed from the running confusion matrix\n",
        "\n",
        "It retrurns the metric dictionary.\n",
        "\n",
        "**Question:** implement the training loop\n",
        "\n",
        "*Note:* do not forget to put the network in training mode\n"
      ],
      "metadata": {
        "id": "KkE4cJ5--6sJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc8yr0ItgZ7q"
      },
      "outputs": [],
      "source": [
        "def train_loop(network, train_dataloader, optimizer, num_classes, device):\n",
        "  network.train()\n",
        "  train_cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "  train_loss = 0\n",
        "  iter_count = 0\n",
        "\n",
        "  t = tqdm.tqdm(train_dataloader, ncols=100)\n",
        "  for batch in t:\n",
        "    batch = to_device(batch, device)\n",
        "    predictions = network(batch)\n",
        "    loss = classif_loss(predictions, batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    cm = conf_matrix(predictions, batch, num_classes)\n",
        "    train_cm += cm\n",
        "    train_loss += loss.item()\n",
        "    iter_count += 1\n",
        "\n",
        "    # batch metrics\n",
        "    metrics = get_metrics(train_cm)\n",
        "    t.set_description_str(f\"Acc: {metrics['accuracy']*100:.1f}, mAcc: {metrics['average_accuracy']*100:.1f}\")\n",
        "\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as before, but for the validation loop.\n",
        "There is no backward step.\n",
        "\n",
        "**Question:** implement the validation loop\n",
        "\n",
        "*Note:* do not forget to put the network in evaluation mode\n",
        "\n",
        "*Note2:* use the inference mode (or no gradient) of pytorch to prevent allocating memory for the gradients."
      ],
      "metadata": {
        "id": "7to5GCI-SUUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU4uTiAN8dWa"
      },
      "outputs": [],
      "source": [
        "def val_loop(network, test_dataloader, num_classes, device):\n",
        "  network.eval()\n",
        "  test_cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "  t = tqdm.tqdm(test_dataloader, ncols=100)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch in t:\n",
        "      batch = to_device(batch, device)\n",
        "      predictions = network(batch)\n",
        "      cm = conf_matrix(predictions, batch, num_classes)\n",
        "      test_cm += cm\n",
        "\n",
        "      # batch metrics\n",
        "      metrics = get_metrics(test_cm)\n",
        "      t.set_description_str(f\"Acc: {metrics['accuracy']*100:.1f}, mAcc: {metrics['average_accuracy']*100:.1f}\")\n",
        "\n",
        "  return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PointNet architecture (at least close to)\n",
        "\n",
        "[PointNet](https://arxiv.org/abs/1612.00593) is one of the first network designed specifically for point processing.\n",
        "The design of PointNet is simple (made of MLPs) with the constraint that the output of the network has to remain invariant to permutation of the points (very important with point clouds).\n",
        "\n",
        "To do so, it uses a pooling function as a permutation invariant aggregation.\n",
        "In practice, they chose a max pooling over all the points.\n",
        "\n",
        "We are interested in the classfication version of the paper.\n",
        "\n",
        "In this practical session, in order to stay lightweight and run on cpu, we simplfy it.\n",
        "\n",
        "[![](https://mermaid.ink/img/pako:eNplkEGLwjAQhf9KmFMFLdi9FVaorTe7lN3bGsGhSZtCmpSYoIv1v2_EqgFzmcz73oOZuUCtGYcUGqlPtUBjyfabKuJftqvsck8Wi9XIPj9GkiVRua1md7j2MNk_2Tpk-S6O4yCYh7DwQRXAIoRZMgExkjKicCjxTCqtZafau0Ocv24WX5cHCo9hwtg0w7tUvEvlS8lluIGcQD6STaSdHZydwRx6bnrsmD_X5WakYAXvOYXUfxlv0ElLgaqrt6Kz-udP1ZBa4_gcjHatgLRBefSdGxhaXnTYGuyf6oDqV-tHf_0HWD54Lw?type=png)](https://mermaid.live/edit#pako:eNplkEGLwjAQhf9KmFMFLdi9FVaorTe7lN3bGsGhSZtCmpSYoIv1v2_EqgFzmcz73oOZuUCtGYcUGqlPtUBjyfabKuJftqvsck8Wi9XIPj9GkiVRua1md7j2MNk_2Tpk-S6O4yCYh7DwQRXAIoRZMgExkjKicCjxTCqtZafau0Ocv24WX5cHCo9hwtg0w7tUvEvlS8lluIGcQD6STaSdHZydwRx6bnrsmD_X5WakYAXvOYXUfxlv0ElLgaqrt6Kz-udP1ZBa4_gcjHatgLRBefSdGxhaXnTYGuyf6oDqV-tHf_0HWD54Lw)\n",
        "\n",
        "The MLPs will be fixed hidden size MLPs, with three layers, batchnorm and ReLU activations (except for the classiication layer).\n",
        "\n",
        "**Question:** fill the PointNet class.\n",
        "\n",
        "*Note* for easy use of the Batchnorm we permute the dimensions of the points at the entry of the `forward` function."
      ],
      "metadata": {
        "id": "0AJe1HwxU0R5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_pDbiemvodj"
      },
      "outputs": [],
      "source": [
        "class PointNet(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels, hidden_channels, 1, bias=False),\n",
        "            torch.nn.BatchNorm1d(hidden_channels),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Conv1d(hidden_channels, hidden_channels, 1, bias=False),\n",
        "            torch.nn.BatchNorm1d(hidden_channels),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Conv1d(hidden_channels, hidden_channels, 1, bias=False),\n",
        "            torch.nn.BatchNorm1d(hidden_channels),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.class_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels, bias=False),\n",
        "            torch.nn.BatchNorm1d(hidden_channels),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels, bias=False),\n",
        "            torch.nn.BatchNorm1d(hidden_channels),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Linear(hidden_channels, out_classes, bias=False),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, data_dict):\n",
        "        points = data_dict[\"points\"]\n",
        "        x = points.permute(0,2,1)\n",
        "        x = self.encoder(x)\n",
        "        x = torch.max(x, dim=-1)[0]\n",
        "        x = self.class_head(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop\n",
        "\n",
        "Finally, we train the network for classification.\n",
        "\n",
        "For the training augmentations, we use a downsampling (to 1024 points), a random rotation and a random scaling.\n",
        "\n",
        "For validation, we use only random downsampling and no further augmentations.\n"
      ],
      "metadata": {
        "id": "KTMV_6gKUxfZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3bXBTQU-ixx"
      },
      "outputs": [],
      "source": [
        "# number of classes in the datasets and device\n",
        "num_classes = 40\n",
        "device = torch.device(\"cpu\")\n",
        "num_epochs = 5 # will train for 5 epochs, that is low, but OK timewise for a practical session\n",
        "\n",
        "# Create the network\n",
        "# network = ...\n",
        "network = PointNet(3,32,40)\n",
        "\n",
        "# Create the optimizer\n",
        "# optimizer = ...\n",
        "optimizer = create_optimizer(network)\n",
        "\n",
        "# Create the transforms for training\n",
        "# train_transforms = ...\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        RandomDecimation(1024),\n",
        "        RandomRotationZ(),\n",
        "        RandomScale(0.9,1.1),\n",
        "    ])\n",
        "\n",
        "# create the validation transforms\n",
        "# val_transforms = ...\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        RandomDecimation(1024),\n",
        "    ]\n",
        "    )\n",
        "\n",
        "# create the train dataset and val dataset\n",
        "train_dataset = Modelnet40Dataset(rootdir=\"modelnet40_ply_hdf5_2048\", files=\"train_files.txt\", transforms=train_transforms)\n",
        "val_dataset = Modelnet40Dataset(rootdir=\"modelnet40_ply_hdf5_2048\", files=\"test_files.txt\", transforms=val_transforms)\n",
        "\n",
        "# create the train and val dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=1)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=1)\n",
        "\n",
        "# loop over the epochs\n",
        "# and train / val the network\n",
        "# ...\n",
        "for epoch in range(num_epochs):\n",
        "  train_loop(network, train_dataloader, optimizer, num_classes, device)\n",
        "  metrics = val_loop(network, val_dataloader, num_classes, device)\n",
        "\n",
        "plt.imshow(metrics[\"confusion_matrix\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test time augmentations\n",
        "\n",
        "Currently, we validate without augmentation, which makes it almost deterministic (there is stochasiticity in the random downsampling).\n",
        "\n",
        "The network has been trained with multiple augmentations, it is possible also use these augmentations at test time, and accumulate the predictions to improve the scores.\n",
        "\n",
        "This is know as *test-time augmentations*.\n",
        "\n",
        "**Important:** this multiple prediction scheme improves the scores, but should not be used as default for comparing the networks. If done, it should (1) be mentioned explicitely and (2) be also reported the score without augmentations.\n",
        "\n",
        "\n",
        "**Question:** Implement the `TTA` class, that takes a transform argument.\n",
        "It applies the transform several times the transform on the points (from the data dictionary) and store the new point clouds in a list.\n",
        "This list replaces the \"points\" field in the data dictionary before being returned.\n"
      ],
      "metadata": {
        "id": "EAa-WRuPUtiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement a test loop with TTA\n",
        "class TTA:\n",
        "\n",
        "  def __init__(self, num_tta, transform):\n",
        "    self.num_tta = num_tta\n",
        "    self.transform = transform\n",
        "\n",
        "  def __call__(self, data_dict):\n",
        "    points = data_dict[\"points\"]\n",
        "    point_list = []\n",
        "    for _ in range(self.num_tta):\n",
        "      aug_data = self.transform({\"points\":points})\n",
        "      point_list.append(aug_data[\"points\"])\n",
        "    points = np.stack(point_list, axis=0)\n",
        "    data_dict[\"points\"] = points\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "039K7xwKUb44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** implement the test loop. The predictions should be summed for a single point cloud."
      ],
      "metadata": {
        "id": "ytBsCWsA9zl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(network, test_dataloader, num_classes, device):\n",
        "  network.eval()\n",
        "  test_cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "  t = tqdm.tqdm(test_dataloader, ncols=100)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch in t:\n",
        "      batch = to_device(batch, device)\n",
        "      sh = batch[\"points\"].shape\n",
        "      batch[\"points\"] = batch[\"points\"].reshape(-1, sh[2], sh[3])\n",
        "      predictions = network(batch)\n",
        "      predictions = predictions.reshape(sh[0], sh[1], -1)\n",
        "      predictions = predictions.mean(dim=1)\n",
        "      cm = conf_matrix(predictions, batch, num_classes)\n",
        "      test_cm += cm\n",
        "\n",
        "      # batch metrics\n",
        "      metrics = get_metrics(test_cm)\n",
        "      t.set_description_str(f\"Acc: {metrics['accuracy']*100:.1f}, mAcc: {metrics['average_accuracy']*100:.1f}\")\n",
        "  return metrics\n",
        "\n",
        "\n",
        "# create the test time transforms\n",
        "test_transforms = TTA(8,Compose(\n",
        "    [\n",
        "        RandomDecimation(1024),\n",
        "        RandomRotationZ(),\n",
        "        RandomScale(0.9,1.1)\n",
        "    ]\n",
        "    ))\n",
        "test_dataset = Modelnet40Dataset(rootdir=\"modelnet40_ply_hdf5_2048\", files=\"test_files.txt\", transforms=test_transforms)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
        "metrics = test_loop(network, test_dataloader, num_classes, device)\n",
        "plt.imshow(metrics[\"confusion_matrix\"])"
      ],
      "metadata": {
        "id": "7L__ARMCUdSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DGCNN Dynamic Graph Convolutional Neural Network\n",
        "\n",
        "[DGCNN](https://arxiv.org/abs/1801.07829) is one of the most used network for geometric deep learning on graphs (or small point clouds).\n",
        "\n",
        "It relies on the computation of a local graph (KNN graph).\n",
        "We will compute the indices of the neighbors of each point.\n",
        "To do so, we can use a transformation.\n",
        "\n",
        "**Question:** fill the `KNNIndices` class, that computes the K-nearest neighbors of all the points in the point cloud using a KDTree. The indices are stored in the dictionary, in the field \"indices\"."
      ],
      "metadata": {
        "id": "WoGcuL_OUmZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KNNIndices:\n",
        "    def __init__(self, k):\n",
        "      self.k = k\n",
        "\n",
        "    def __call__(self, data_dict):\n",
        "      points = data_dict[\"points\"]\n",
        "      tree = KDTree(points)\n",
        "      _, indices = tree.query(points, self.k)\n",
        "      data_dict[\"indices\"] = torch.tensor(indices, dtype=torch.long)\n",
        "      return data_dict\n",
        "\n",
        "\n",
        "points = data[0].clone()\n",
        "label = labels[0]\n",
        "data_dict = {\"points\":points, \"labels\":label}\n",
        "transform = KNNIndices(32)\n",
        "data_dict = transform(data_dict)\n",
        "colors = np.zeros(points.shape[0], dtype=np.uint8)\n",
        "colors[data_dict[\"indices\"][0]] = 1\n",
        "colors[0] = 2\n",
        "point_cloud_visu(points, colors)\n"
      ],
      "metadata": {
        "id": "fk1ZZaLeKhz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** implement the function `knn_gather`. The function use the `gather` function of torch, to aggregate the features of the K-nearest neighbors of each point into a tensor.\n",
        "\n",
        "Inputs:\n",
        "* `x` of shape [B c N]\n",
        "* `indices` of shape [B N K], to be shaped in [B 1 N*K] and then expanded to [B C N*K]\n",
        "Outputs:\n",
        "* x_knn of shape [B C, N, K]"
      ],
      "metadata": {
        "id": "lg2ca5YvRlur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_gather(x, indices):\n",
        "    indices_ = indices.view(indices.shape[0], 1, -1).expand(-1, x.shape[1], -1) # [B C N*K]\n",
        "    x_knn = torch.gather(x, dim=2, index=indices_)\n",
        "    x_knn = x_knn.reshape(x_knn.shape[0], x_knn.shape[1], x.shape[2], -1)\n",
        "    return x_knn # shape [B C N K]"
      ],
      "metadata": {
        "id": "Dp4uBo4qSgjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** implement the function `features_from_graph` that computes the input the features of the edge convolution from DGCNN.\n",
        "The function:\n",
        "1. computes the knn features ($x_j$)\n",
        "2. the edge features ($x_i - x_j$)\n",
        "3. concatenate the edge features with the node features ($[x_i, x_i-x_j]$)\n",
        "Output shape is [C 2*C N K]"
      ],
      "metadata": {
        "id": "6SGBOibYSirI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def features_from_graph(x, indices):\n",
        "    x_edge = knn_gather(x, indices) # [B, C, N, K]\n",
        "    x_edge = x_edge - x.unsqueeze(-1)\n",
        "    x = torch.cat([x_edge, x.unsqueeze(-1).repeat(1,1,1,x_edge.shape[-1])], dim=1)\n",
        "    return x # shape [B, 2*C, N, K]\n"
      ],
      "metadata": {
        "id": "qN8PDXI7TISB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** fill the DGCNN convolutional module.\n",
        "The forward pass:\n",
        "1. computes thes features from the graph\n",
        "2. applies a linear layer (a Conv2d, with kernel size 1)\n",
        "3. applies a batchnorm\n",
        "4. uses ReLU as activation\n",
        "5. compute the max over the K values\n",
        "Output is of size [B C N]"
      ],
      "metadata": {
        "id": "3QR35wmKTNQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DGCNNConv(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = torch.nn.Conv2d(2*in_channels, out_channels, 1, bias=False)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.act = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x, indices):\n",
        "        # compute the edge features\n",
        "        x = features_from_graph(x, indices)\n",
        "        x = self.act(self.bn(self.conv(x)))\n",
        "        x = torch.max(x, dim=-1)[0]\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NzYUVx4aTKxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** fill the DGCNN network and train the model. For simplicity, we only train for 5 epochs and use 2 convolution block + 1 linear layer for the classification head."
      ],
      "metadata": {
        "id": "QJ8_khibT6dn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCTphWnlVW5_"
      },
      "outputs": [],
      "source": [
        "class DGCNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = DGCNNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = DGCNNConv(hidden_channels, hidden_channels)\n",
        "        self.classifier_head = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data_dict):\n",
        "        # compute the indices\n",
        "        x = data_dict[\"points\"].permute(0,2,1)\n",
        "        indices = data_dict[\"indices\"]\n",
        "\n",
        "        # through the encoder\n",
        "        x = self.conv1(x, indices)\n",
        "        x = self.conv2(x, indices)\n",
        "\n",
        "        # through the classifier\n",
        "        x = torch.max(x, dim=-1)[0]\n",
        "        x = self.classifier_head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "num_classes = 40\n",
        "network = DGCNN(3,32,40)\n",
        "optimizer = create_optimizer(network)\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        RandomDecimation(1024),\n",
        "        RandomRotationZ(),\n",
        "        RandomScale(0.9,1.1),\n",
        "        KNNIndices(16),\n",
        "    ])\n",
        "\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        RandomDecimation(1024),\n",
        "        KNNIndices(16),\n",
        "    ]\n",
        "    )\n",
        "\n",
        "train_dataset = Modelnet40Dataset(rootdir=\"modelnet40_ply_hdf5_2048\", files=\"train_files.txt\", transforms=train_transforms)\n",
        "val_dataset = Modelnet40Dataset(rootdir=\"modelnet40_ply_hdf5_2048\", files=\"test_files.txt\", transforms=val_transforms)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=1)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loop(network, train_dataloader, optimizer, num_classes, device)\n",
        "  val_loop(network, val_dataloader, num_classes, device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}