{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18nR8zB1ObJY"
      },
      "source": [
        "# Point cloud segmentation\n",
        "\n",
        "In this practical session, we will train a WaffleIron network for point cloud segmentation.\n",
        "\n",
        "The original research paper is [here](https://arxiv.org/abs/2301.10100). The network is originally designed spcifically for outdoor lidar semantic segmentation.\n",
        "\n",
        "![](https://github.com/valeoai/WaffleIron/raw/master/illustration.png)\n",
        "\n",
        "\n",
        "For the sake of GPU resources and computational time, we will create a lighter variation of the model to do part segmentation on the [ShapeNet dataset](https://shapenet.org/).\n",
        "\n",
        "The objective is not to train a state-of-the-art model, but to grasp the essentials of training for semantic segmentation (with the small modifications specific to part segmentation).\n",
        "\n",
        "As opposed to previous classification session where, given a point cloud $P$ the objective was to predict the class $c_P$, here the objective is to predict a class for each point in the point cloud: $\\{c_p \\forall p \\in P \\}$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The notebook follows a classic setup for classification pipeline.\n",
        "1. notebook setup and data preparation\n",
        "2. data augmentation creation\n",
        "3. metrics\n",
        "4. training and validation loop\n",
        "5. network definition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook and data setup\n",
        "\n",
        "We first import the librairies needed for the practical session."
      ],
      "metadata": {
        "id": "wlfVyOMZVcsM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQMAGBi7OK7q"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.graph_objects as go # for visualization\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.spatial import KDTree\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next block download the shapenet dataset for part segmentation, unzip the archive and rename the folder as `shape_data`."
      ],
      "metadata": {
        "id": "Z9HSb1VXP65N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9-tZygLUeZg"
      },
      "outputs": [],
      "source": [
        "hf_hub_download(repo_id=\"wangps/shapenet_segmentation\", filename=\"shapenetcore_partanno_segmentation_benchmark_v0_normal.zip\", repo_type=\"dataset\", cache_dir=\".\")\n",
        "!unzip -qq ./datasets--wangps--shapenet_segmentation/snapshots/dbde146b974e1fc8628b47b1b1c4e50d8bc1a2ef/shapenetcore_partanno_segmentation_benchmark_v0_normal\n",
        "!mv shapenetcore_partanno_segmentation_benchmark_v0_normal shape_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define the variables for path to the files containing the lists of shape filenames for training and validation split."
      ],
      "metadata": {
        "id": "yIcET2tcoO17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_files = \"shape_data/train_test_split/shuffled_train_file_list.json\"\n",
        "val_files = \"shape_data/train_test_split/shuffled_val_file_list.json\""
      ],
      "metadata": {
        "id": "Qsr8ao02eLIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, we define the function for visualization."
      ],
      "metadata": {
        "id": "Le2O36xoopSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display the point cloud\n",
        "def point_cloud_visu(pts, cls=None):\n",
        "\n",
        "    marker_dict = dict(size=3,\n",
        "                            color=cls,\n",
        "                            )\n",
        "\n",
        "    if (cls is not None) and (\n",
        "        len(cls.shape)==1 or\n",
        "        (len(cls.shape)==2 and (cls.shape[1]==1))):\n",
        "      marker_dict[\"colorscale\"] = 'Viridis'\n",
        "\n",
        "    fig = go.Figure(\n",
        "        data=[\n",
        "            go.Scatter3d(\n",
        "                x=pts[:,0], y=pts[:,1], z=pts[:,2],\n",
        "                mode='markers',\n",
        "                marker=marker_dict,\n",
        "            )\n",
        "        ],\n",
        "        layout=dict(\n",
        "            scene=dict(\n",
        "                xaxis=dict(visible=False),\n",
        "                yaxis=dict(visible=False),\n",
        "                zaxis=dict(visible=False),\n",
        "                aspectmode=\"data\", #this string can be 'data', 'cube', 'auto', 'manual'\n",
        "                #a custom aspectratio is defined as follows:\n",
        "                aspectratio=dict(x=1, y=1, z=0.95)\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "_6gODwSxbCxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create two functions:\n",
        "- `load_files`: function for loading the files for given split\n",
        "- `load_data`: function for loading one point cloud from a filename\n",
        "\n",
        "Points are converted from numpy to tensors."
      ],
      "metadata": {
        "id": "Xb0ypfk4pwk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_files(files):\n",
        "  return json.load(open(files))\n",
        "\n",
        "def load_data(filename):\n",
        "  data = np.loadtxt(filename+\".txt\")\n",
        "  points = data[:,[0,2,1]]\n",
        "  labels = data[:,6]\n",
        "  points = torch.tensor(points, dtype=torch.float)\n",
        "  labels = torch.tensor(labels, dtype=torch.long)\n",
        "  return points, labels\n",
        "\n",
        "files = load_files(train_files)\n",
        "points, labels = load_data(files[1])\n",
        "point_cloud_visu(points, labels)"
      ],
      "metadata": {
        "id": "3_zEGtGZdQHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations\n",
        "\n",
        "As in the previous practical session, we first define the transformations used in processing pipeline to create / augment the point clouds.\n",
        "\n",
        "#### Random decimation\n",
        "First tranformation is the random decimation of the point clouds. Note that as opposed to classification, the labels must be decimated as well.\n",
        "\n",
        "**Question 1:** Fill the `RandomDecimation` class."
      ],
      "metadata": {
        "id": "zfL8SxNnlwhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomDecimation:\n",
        "  def __init__(self, num_points) -> None:\n",
        "    # fill here\n",
        "    pass\n",
        "\n",
        "  def __call__(self, data_dict):\n",
        "    # fill here\n",
        "    return data_dict\n",
        "\n",
        "files = load_files(train_files)\n",
        "points, labels = load_data(files[1])\n",
        "data_dict = {\"points\":points, \"labels\":labels}\n",
        "transform = RandomDecimation(512)\n",
        "data_dict = transform(data_dict)\n",
        "points_t1 = data_dict[\"points\"] + torch.tensor([1.,0,0])\n",
        "points = torch.cat([points, points_t1], dim=0)\n",
        "labels = torch.cat([labels, data_dict[\"labels\"]])\n",
        "point_cloud_visu(points, labels)"
      ],
      "metadata": {
        "id": "DotZvIyKeYqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random rotation around the $z$-axis\n",
        "\n",
        "The random rotation around the $z$, draws a random angle and the operate the corresponding rotation. Note that, as the order of the points is not changed, there is no need to modify the labels.\n",
        "\n",
        "**Question 2:** fill the `RandomRotationZ` class."
      ],
      "metadata": {
        "id": "IdPpe1pllawo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomRotationZ:\n",
        "  def __call__(self, data_dict):\n",
        "    # fill here\n",
        "    return data_dict\n",
        "\n",
        "files = load_files(train_files)\n",
        "points, labels = load_data(files[1])\n",
        "data_dict = {\"points\":points, \"labels\":labels}\n",
        "transform = RandomRotationZ()\n",
        "data_dict = transform(data_dict)\n",
        "points_t1 = data_dict[\"points\"] + torch.tensor([1.,0,0])\n",
        "points = torch.cat([points, points_t1], dim=0)\n",
        "labels = torch.cat([labels, data_dict[\"labels\"]])\n",
        "point_cloud_visu(points, labels)"
      ],
      "metadata": {
        "id": "_8rRHNMeffgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random scaling\n",
        "\n",
        "The random scaling draws a scale factor in the range $[s_\\min, s_\\max]$ and multiply the coordinates of the point cloud by this factor. Note that, as the order of the points is not changed, there is no need to modify the labels.\n",
        "\n",
        "**Question 3** fill the `RandomScale` class."
      ],
      "metadata": {
        "id": "Z7_qC_k1lc1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomScale:\n",
        "    def __init__(self, scale_min, scale_max):\n",
        "      # fill here\n",
        "      pass\n",
        "\n",
        "    def __call__(self, data_dict):\n",
        "      # fill here\n",
        "      return data_dict\n",
        "\n",
        "files = load_files(train_files)\n",
        "points, labels = load_data(files[1])\n",
        "data_dict = {\"points\":points, \"labels\":labels}\n",
        "transform = RandomScale(0.1,2)\n",
        "data_dict = transform(data_dict)\n",
        "points_t1 = data_dict[\"points\"] + torch.tensor([1.,0,0])\n",
        "points = torch.cat([points, points_t1], dim=0)\n",
        "labels = torch.cat([labels, data_dict[\"labels\"]])\n",
        "point_cloud_visu(points, labels)"
      ],
      "metadata": {
        "id": "FRl54EI4fsq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Project to a plane\n",
        "\n",
        "WaffeIron consists of blocks made of two sub-blocks (see figure).\n",
        "\n",
        "The first one is a spatial mixing block, which first projec the points to one of the principal planes ($(x,y), (y,z), (z,x)$) and run $3\\times3$ convolution on the generated images to finally reproject the features to the points.\n",
        "\n",
        "The second one is a fetaure mixing blocks, very similar to the MLP part of the PointNet network. It consists in applying a small MLP on the point features.\n",
        "\n",
        "Both blocks use a residual connection.\n",
        "\n",
        "In order to do this projection, we will compute the projection indices in the three planes.\n",
        "\n",
        "The projection to planes takes as parameter the image size that will be generated (it will a square one).\n",
        "\n",
        "In the `__call__` method, we iterate over the dimension list (`self.dims`) that are couples of the dimensions we keep to generate the projection.\n",
        "\n",
        "The function then:\n",
        "- rescales the 2D points to $[0, \\text{self.im_size}]$ (we consider for constant projections that the bounds of the points are $[-0.5,0.5]$)\n",
        "- converts the 2D points to intergral coordinates\n",
        "- clip the coordinates to $[0, \\text{self.im_size}-1]$ (because to scaling there may points that are outside the image)\n",
        "- convert the 2D to 1D-coordinate ($(x,y) \\rightarrow x * \\text{self.im_size} + y$)\n",
        "\n",
        "It returns the 3 set of coordinates (one for each plane).\n",
        "\n",
        "**Question 4**: fill the `ProjectionToPlane` class."
      ],
      "metadata": {
        "id": "MohjHis2lkrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know if your code correctly working, here is the results of the sample code.\n",
        "\n",
        "![téléchargement.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "hi0cWyIkkSIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionToPlane:\n",
        "  def __init__(self, im_size):\n",
        "    self.im_size = im_size\n",
        "    self.dims = [[0,1], [1,2], [0,2]]\n",
        "\n",
        "  def __call__(self, data_dict):\n",
        "    # fill here\n",
        "    return data_dict\n",
        "\n",
        "\n",
        "files = load_files(train_files)\n",
        "points, labels = load_data(files[1])\n",
        "data_dict = {\"points\":points, \"labels\":labels}\n",
        "im_size = 32\n",
        "transform = ProjectionToPlane(im_size)\n",
        "data_dict = transform(data_dict)\n",
        "im = torch.zeros((im_size**2), dtype=torch.long)\n",
        "im[data_dict[\"indices\"][0]] = 1\n",
        "im = im.reshape((im_size,im_size))\n",
        "plt.imshow(im)\n"
      ],
      "metadata": {
        "id": "9Of3Q0Phg8cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Composition\n",
        "\n",
        "As for the classification practical session, we need a class that will do all the transformation by iterating over a list given as parameter.\n",
        "\n",
        "**Question 5** fill the `Compose` class."
      ],
      "metadata": {
        "id": "-flSIPiFloU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Compose:\n",
        "  def __init__(self, transform_list):\n",
        "    # fill here\n",
        "    pass\n",
        "\n",
        "  def __call__(self, data_dict):\n",
        "    # fill here\n",
        "    return data_dict\n",
        "\n",
        "transform = Compose(\n",
        "    [\n",
        "        RandomDecimation(1024),\n",
        "        RandomRotationZ(),\n",
        "        RandomScale(0.9,1.1),\n",
        "        ProjectionToPlane(32),\n",
        "    ])\n",
        "\n",
        "files = load_files(train_files)\n",
        "points, labels = load_data(files[1])\n",
        "data_dict = {\"points\":points, \"labels\":labels}\n",
        "data_dict = transform(data_dict)\n",
        "points_t1 = data_dict[\"points\"] + torch.tensor([1.,0,0])\n",
        "points = torch.cat([points, points_t1], dim=0)\n",
        "labels = torch.cat([labels, data_dict[\"labels\"]])\n",
        "point_cloud_visu(points, labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "u-tTO0MOghOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "**Question 6:** implement the dataset for ShapeNet part.\n",
        "- the argument is `split` (train or val), which allows loading the data for the train or the val split.\n",
        "- `__len__` return the number of samples in the split\n",
        "- `forward` loads (for a sample id `idx`) the corresponding data, applies a transformation if there is one (should have been stored in `self.transform`), and returns dictionary with the fields `points` and `labels`."
      ],
      "metadata": {
        "id": "GxuRKhDSGSKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataloader\n",
        "class ShapeNetPart(Dataset):\n",
        "\n",
        "  def __init__(self, split, transforms=None) -> None:\n",
        "    super().__init__()\n",
        "    # fill here\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    # fill here\n",
        "    pass\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # fill here\n",
        "    return {}\n",
        "\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        RandomDecimation(512),\n",
        "        RandomRotationZ(),\n",
        "        RandomScale(0.9,1.1),\n",
        "        ProjectionToPlane(32),\n",
        "    ])\n",
        "\n",
        "train_dataset = ShapeNetPart(\"train\", transforms=train_transforms)\n",
        "data_dict = train_dataset[1]\n",
        "point_cloud_visu(data_dict[\"points\"], data_dict[\"labels\"])"
      ],
      "metadata": {
        "id": "1pSdVHT4ZGrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network definition\n",
        "\n",
        "In this section, we define the WaffleIron network.\n",
        "It is composed of three main component.\n",
        "- a point embedding\n",
        "- a backbone, composed of sucession of WaffleIron blocks\n",
        "- a classification head\n",
        "\n",
        "#### Point embedding\n",
        "\n",
        "In the original implementation, the point embedding is a DGCNN block.\n",
        "In this session, we replace it by a simple linear layer (implemented with 1D-convolution).\n",
        "\n",
        "**Question 7:** fill the `Embedding` class."
      ],
      "metadata": {
        "id": "8DY-LXrInKUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(torch.nn.Module):\n",
        "  def __init__(self, in_channel: int, out_channel: int) -> None:\n",
        "    super().__init__()\n",
        "    # fill here\n",
        "  def forward(self, x):\n",
        "    # x dimension is [B C N]\n",
        "    # fill here\n",
        "    return x"
      ],
      "metadata": {
        "id": "rdWCSz01n1kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classification head\n",
        "\n",
        "Similarly for the classification head, we also use a single linear layer (implemented with a Conv1d).\n",
        "It is possible to use more complex heads, but to limit the computational requirements, we limit ourselves to the simplest one.\n",
        "\n",
        "**Question 8:** fill the `ClassificationHead` class."
      ],
      "metadata": {
        "id": "mA0WqrC116QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHead(torch.nn.Module):\n",
        "  def __init__(self, in_channel: int, out_channel: int) -> None:\n",
        "    super().__init__()\n",
        "    # fill here\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x dimension is [B C N]\n",
        "    # fill here\n",
        "    return x"
      ],
      "metadata": {
        "id": "re8MbPYMn2t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Channel mixing\n",
        "\n",
        "The channel mixing block correspond to the right part of the $\\times L$ series of blocks.\n",
        "This block operates per-point operations:\n",
        "- a batchnorm\n",
        "- an MLP (two layers with ReLU activation)\n",
        "- a residual connection with the output\n",
        "During the whole process the feature dimensions stays equal to $C$ (`channels` variable in the code).\n",
        "\n",
        "**Question 9:** fill the `ChannelMixing` class."
      ],
      "metadata": {
        "id": "9qWbb1Ao2dpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelMixing(torch.nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    # fill here\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # x dimension is [B C N]\n",
        "    # fill here\n",
        "    return x\n",
        "\n",
        "print(data_dict.keys())\n",
        "x = data_dict[\"points\"].unsqueeze(0).permute(0,2,1)\n",
        "ids = data_dict[\"indices\"].unsqueeze(0)\n",
        "print(ids.shape, x.shape)\n",
        "net = ChannelMixing(3)\n",
        "print(net(x).shape)"
      ],
      "metadata": {
        "id": "XjajvMevt0sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spatial mixing\n",
        "\n",
        "Spatial mixing (WI block on the figure) is the core of the method.\n",
        "The objective of this block (as in [MLP-Mixer](https://arxiv.org/abs/2105.01601)) is to create relations between adjacent points.\n",
        "One way would be to setup a point convolution layer, but that is costly.\n",
        "Instead, points are projected on a plane where a small convolutional network is applied (mixing), then we operate an un-projection operation to get the features back to the points.\n",
        "\n",
        "The spatial mixing operates several operation on the input:\n",
        "1. a batchnorm is applied on the input\n",
        "2. the indices (shape $[B,N]$) are expanded to shape $[B, C, N]$.\n",
        "3. the points are projected on the planes using the indices (computed in the augmentations), it fills a grid of size $[B, C, G^2]$, where $G$ is the grid size. Use the `scatter_reduce_` function of torch [DOCS](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_) (we do an average reduction).\n",
        "4. the grid is reshaped to $[B, C, G, G]$\n",
        "5. The grid goes to the convolutional layer made of 2 $3 \\times 3$ convolutional layers, with a ReLU in between.\n",
        "6. the grid is reshaped to $[B, C, G^2]$\n",
        "7. the features are un-projected to the points using a `torch.gather` [DOCS](https://pytorch.org/docs/stable/generated/torch.gather.html), that is the inverse operation of the scattering.\n",
        "8. we apply a residual connection with the input.\n",
        "\n",
        "\n",
        "**Question 10:** fill the `SpatialMixing` class."
      ],
      "metadata": {
        "id": "gkaZ_uff2iUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialMixing(torch.nn.Module):\n",
        "  def __init__(self, channels: int, grid_size) -> None:\n",
        "    super().__init__()\n",
        "    # fill here\n",
        "\n",
        "  def forward(self, x: torch.Tensor, indices: torch.Tensor):\n",
        "    # x dimension is [B C N]\n",
        "    # indices dimension is [B, N]\n",
        "    # fill here\n",
        "    return x\n",
        "\n",
        "print(data_dict.keys())\n",
        "x = data_dict[\"points\"].unsqueeze(0).permute(0,2,1)\n",
        "ids = data_dict[\"indices\"].unsqueeze(0)\n",
        "print(ids.shape, x.shape)\n",
        "net = SpatialMixing(3,32)\n",
        "print(net(x, ids[:,0]).shape)\n"
      ],
      "metadata": {
        "id": "5dS5hNtPxwEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block\n",
        "\n",
        "The `WaffleIronBlock` class is simply a spatial mixing block followed by the channel mixing block.\n",
        "\n",
        "**Question 11:** fill the `WaffleIronBlock` class."
      ],
      "metadata": {
        "id": "zqL3qojZ2lCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaffleIronBlock(torch.nn.Module):\n",
        "  def __init__(self, channels, grid_size):\n",
        "    super().__init__()\n",
        "    # fill here\n",
        "\n",
        "  def forward(self, x: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n",
        "    # x dimension is [B C N]\n",
        "    # indices dimension is [B, N]\n",
        "    # fill here\n",
        "    return x"
      ],
      "metadata": {
        "id": "SUCv2F9dupe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Network\n",
        "\n",
        "Finally, we buid the the network.\n",
        "It composed of:\n",
        "- a `PointEmbedding`\n",
        "- `block_num` `WaffleIronBlock` blocks (you can create a list and use the `torch.nn.ModuleList` [DOCS](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html)\n",
        "- a classification head\n",
        "\n",
        "The forward method:\n",
        "- gets the points from the data dictionary (shape $[B, N, C]$)\n",
        "- permutes to the dimensions to get shape $[B, C, N]$\n",
        "- applies the embedding\n",
        "- applies the blocks, with the indices corresponding to the plane to be projected (indices are of shape $[B, P, N]$) so the indices should be selected along the $P$ dimension with $p = i \\% P $, where i in the index of the block.\n",
        "- applies the classification head.\n",
        "\n",
        "**Question 12:** fill the `WaffleIron` class."
      ],
      "metadata": {
        "id": "q8CiXDLn2mtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaffleIron(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, block_num, grid_size):\n",
        "    super().__init__()\n",
        "    # fill here\n",
        "\n",
        "\n",
        "  def forward(self , data_dict):\n",
        "    # fill here\n",
        "    return x\n",
        "\n",
        "\n",
        "network = WaffleIron(3,32,50, 6, 32)\n",
        "train_dataset = ShapeNetPart(\"train\", transforms=train_transforms)\n",
        "data_dict = train_dataset[1]\n",
        "data_dict[\"points\"] = data_dict[\"points\"].unsqueeze(0)\n",
        "data_dict[\"indices\"] = data_dict[\"indices\"].unsqueeze(0)\n",
        "predictions = network(data_dict)\n",
        "print(predictions.shape)\n"
      ],
      "metadata": {
        "id": "Npl21dcft9ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "\n",
        "We now define the metrics.\n",
        "\n",
        "**Question 13:** fill the `conf_matrix` function."
      ],
      "metadata": {
        "id": "ogwEP6rblp8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conf_matrix(predictions, batch, num_classes):\n",
        "  # fill here\n",
        "  return 0\n",
        "\n",
        "predictions = torch.tensor([[0.1, 0.2, 0.7, 0.0],\n",
        "               [0.0, 0.6, 0.1, 0.3],\n",
        "               [0.8, 0.2, 0.0, 0.0],\n",
        "               [0.9, 0.0, 0.0, 0.1],\n",
        "               [0.1, 0.2, 0.0, 0.6],\n",
        "               [0.1, 0.4, 0.5, 0.0],\n",
        "               ], dtype=torch.float)\n",
        "labels = torch.tensor([2,2,0,0,3,1])\n",
        "batch = {\"labels\": labels}\n",
        "cm = conf_matrix(predictions, batch, 4)\n",
        "plt.imshow(cm)"
      ],
      "metadata": {
        "id": "24Z_qOGnymh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Global accuracy\n",
        "**Question 14:** fill the `global_accuracy` function. (From example, it should output 0.67)"
      ],
      "metadata": {
        "id": "08vicn09mfCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def global_accuracy(cm):\n",
        "  # fill here\n",
        "  return 0\n",
        "\n",
        "print(global_accuracy(cm))"
      ],
      "metadata": {
        "id": "A_GnUe63mjpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Accuracy per class\n",
        "\n",
        "**Question 15:** fill the `accuracy_per_class` function. (example should give [1.  0.  0.5 1. ])"
      ],
      "metadata": {
        "id": "_2L566hMm-NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_per_class(cm):\n",
        "  # fill here\n",
        "  return 0\n",
        "\n",
        "print(accuracy_per_class(cm))"
      ],
      "metadata": {
        "id": "4C81XemommLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IoU and IoU per class\n",
        "\n",
        "**Question 16:** fill the `iou` function, it returns:\n",
        "- the average IoU (to compute the average, we remove the classes that are not present)\n",
        "- the iou per class (not present classes are filled with the average IoU)\n",
        "\n",
        "Example should return mIoU: 0.58, IoU per class (1., 0., 0.3, 1.)."
      ],
      "metadata": {
        "id": "RxVCaCR9nBB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(cm):\n",
        "  # fill here\n",
        "  return 0, [0,0]\n",
        "\n",
        "print(iou(cm))"
      ],
      "metadata": {
        "id": "FqUX3cUBmoEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 17**: implement the get metrics function which returns a dictionary with the accuracy, the average accuracy, the accuracy per class, the IoU and the IoU per class, and the confusion matrix."
      ],
      "metadata": {
        "id": "vyYlpfqsnD0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(cm):\n",
        "  # fill here\n",
        "  return {}\n",
        "\n",
        "print(get_metrics(cm))"
      ],
      "metadata": {
        "id": "AgqUm2RGy06v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions\n",
        "\n",
        "We provide few functions, useful for training:\n",
        "- `classif_loss`: returns the cross-entropy loss\n",
        "- `create_optimizer`: creates and returns an AdamW optimizer of the network parameters\n",
        "- `to_device`: move the data of dictionary, a list or a tensor to the given device."
      ],
      "metadata": {
        "id": "YN9Y9VNBm9Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classif_loss(predictions, batch):\n",
        "  labels = batch[\"labels\"]\n",
        "  loss = torch.nn.functional.cross_entropy(predictions, labels)\n",
        "  return loss\n",
        "\n",
        "def create_optimizer(network):\n",
        "  optimizer = torch.optim.AdamW(network.parameters(), lr=1e-3)\n",
        "  return optimizer\n",
        "\n",
        "def to_device(batch, device):\n",
        "  if isinstance(batch, torch.Tensor):\n",
        "    return batch.to(device)\n",
        "  elif isinstance(batch, list):\n",
        "    batch_ = []\n",
        "    for elem in batch:\n",
        "      if isinstance(elem, torch.Tensor):\n",
        "        elem = elem.to(device)\n",
        "      batch_.append(elem)\n",
        "    return batch_\n",
        "  elif isinstance(batch, dict):\n",
        "    batch_ = {}\n",
        "    for key, elem in batch.items():\n",
        "      if isinstance(elem, torch.Tensor):\n",
        "        elem = elem.to(device)\n",
        "      batch_[key] = elem\n",
        "    return batch_\n",
        "  else:\n",
        "    raise ValueError(\"unknow batch type\")"
      ],
      "metadata": {
        "id": "EbALUYmdy659"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Label masking\n",
        "\n",
        "One particularity of the part segmentation task, with respect to the semantic segmentation task, is that one evaluates only the segmentation for classes corresponding to the object.\n",
        "\n",
        "That means that even though we train for 50 classes (the total number of part classes in ShapeNet), we evaluate on 2 to 5 classes (e.g., a plane is 4 classes: main, wings, tail, reactor; a table is 2 classes: legs and top...).\n",
        "\n",
        "To do so, we will first generate a mask (see image below), with False values for the class ids corresponding to the object, True for the class ids not corresponding to the object.\n",
        "\n",
        "We give the start index of each class (to which we added a final 50).\n",
        "\n",
        "**Question 18:** fill the function `create_mask`. It returns the mask and a vector called `label_to_mask` which contains the for given label which line of the mask should be used.\n",
        "\n",
        "\n",
        "*Note:* mask size `torch.Size([16, 50])`\n",
        "*Note 2:* `label_to_mask` value `[ 0,  0,  0,  0,  1,  1,  2,  2,  3,  3,  3,  3,  4,  4,  4,  4,  5,  5,\n",
        "         5,  6,  6,  6,  7,  7,  8,  8,  8,  8,  9,  9, 10, 10, 10, 10, 10, 10,\n",
        "        11, 11, 12, 12, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15]`\n",
        "*Note 3:* a plot of the mask should give the image below.\n"
      ],
      "metadata": {
        "id": "eZBJlRz9SxGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "ejtlqaBmIqE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask():\n",
        "  # fill here\n",
        "  return 0, 0\n",
        "\n",
        "mask, label_to_mask = create_mask()\n",
        "\n",
        "print(mask.shape)\n",
        "print(label_to_mask)\n",
        "plt.imshow(mask)"
      ],
      "metadata": {
        "id": "U3jpu-jc5PRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pruning the non object classes\n",
        "\n",
        "Then given a mask calulated before, and a set of predictions, we mask the predictions of the non-object classes.\n",
        "\n",
        "The function:\n",
        "- computes the minimum label for object in the batch (from the groung truth labels contained in `batch`)\n",
        "- gets the corresponding line in the mask\n",
        "- applies the mask and set `-1e6` value for the non class object (that force a very low softmax score, thus ignoring these classes)\n",
        "- returns the update predictions.\n",
        "\n",
        "**Question 19:** fill the `prune_non_object_classes` function."
      ],
      "metadata": {
        "id": "BQuRd7QDUo_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_non_object_classes(predictions, batch, mask, label_to_mask):\n",
        "  # fill here\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "YiPlLJCz5Uu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation loops\n",
        "\n",
        "**Question 20:** code the training loop which perfroms one epoch training. It trains the network (do not forget to put the network in train mode), updates the predictions via `prune_non_object_classes`, maintains a confusion matrix with the predictions for the epochs, and returns the metrics."
      ],
      "metadata": {
        "id": "rEdZNXXAVkkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(network, train_dataloader, optimizer, num_classes, device):\n",
        "  # fill here\n",
        "  return 0"
      ],
      "metadata": {
        "id": "_bc_7Xk_zA36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 21:** code the validation loop which perfroms the evaluation for the validation set. Do not forget to put the network in eval mode. It updates the predictions via `prune_non_object_classes`, maintains a confusion matrix with the predictions for the epochs, and returns the metrics."
      ],
      "metadata": {
        "id": "5_0nylmhWFqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def val_loop(network, test_dataloader, num_classes, device):\n",
        "  # fill here\n",
        "  return 0"
      ],
      "metadata": {
        "id": "gcmgePJ8zCGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the network\n",
        "\n",
        "**Question 22:** train the Waffleiron network. Do not forget to use the `inference_mode` for validation."
      ],
      "metadata": {
        "id": "x7ix4LOFWQKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of classes in the datasets and device\n",
        "num_classes = 50\n",
        "device = torch.device(\"cpu\")\n",
        "num_epochs = 5 # will train for 5 epochs, that is low, but OK timewise for a practical session\n",
        "\n",
        "# Create the network ( with 3 blocks, hidden size 32, grid_size 16, and 50 classes for segmentation)\n",
        "# network = ...\n",
        "\n",
        "# Create the optimizer\n",
        "# optimizer = ...\n",
        "\n",
        "# Create the transforms for training --> grid size 16\n",
        "# train_transforms = ...\n",
        "\n",
        "# create the validation transforms --> grid size 16\n",
        "# val_transforms = ...\n",
        "\n",
        "# create the train dataset and val dataset\n",
        "# train_dataset = ...\n",
        "# val_dataset = ...\n",
        "\n",
        "# create the train and val dataloaders\n",
        "# train_dataloader = ...\n",
        "# val_dataloader = ...\n",
        "\n",
        "# loop over the epochs\n",
        "# and train / val the network\n",
        "# ...\n",
        "\n",
        "\n",
        "plt.imshow(metrics[\"confusion_matrix\"])"
      ],
      "metadata": {
        "id": "ud8lmqnAzHED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result visualization\n",
        "\n",
        "Looking at the confusion matrix is nice but visual inspection of the results is also needed.\n",
        "\n",
        "**Question 23:** predict the labels for the first few point clouds in the validation dataloader, apply translations to them and visualize them along with the predicted labels."
      ],
      "metadata": {
        "id": "iRh5nb7iWcxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict for visualization"
      ],
      "metadata": {
        "id": "5UidwOFo-Yqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}