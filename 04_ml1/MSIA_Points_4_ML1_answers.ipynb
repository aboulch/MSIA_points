{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJy8u4gu1ukp"
      },
      "source": [
        "# MSIA - Points - 4 - Machine learning\n",
        "\n",
        "This practical session is our first machine learning session.\n",
        "We target semantic segmentation for outdoor scenes.\n",
        "We will compute local descriptors in a point cloud and use these descriptors to estimate the point labels.\n",
        "\n",
        "We will roughly follow the procedure described in *Semantic Classification of 3D Point Clouds with Multiscale Spherical Neighborhoods*, Thomas et al.\n",
        "\n",
        "The point clouds are sub point clouds from [NPM3D](https://npm3d.fr/), a benchmark suite for lidar semantic segmentation.\n",
        "If interested, please login to the benchmark and test your ideas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP3Z-7BaEZeu"
      },
      "source": [
        "## Notebook setup\n",
        "\n",
        "For point cloud loading we use here `plyfile` library, that can load a point cloud in ply format with its attributes (here a class label).\n",
        "\n",
        "Then, we download the point cloud we will work with. The point cloud can be directly donwloaded from the course [github]( https://github.com/aboulch/MSIA_points) or using the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95tnqZMmvcM5"
      },
      "outputs": [],
      "source": [
        "# install the missing package\n",
        "!pip install plyfile\n",
        "\n",
        "# load the point clouds collection of point clouds\n",
        "!wget https://github.com/aboulch/MSIA_points/releases/download/v0.0.0/Lille1_1_sub_train.ply\n",
        "!wget https://github.com/aboulch/MSIA_points/releases/download/v0.0.0/Lille1_1_sub_val.ply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPS072KNFObm"
      },
      "source": [
        "Finally, we import the libraries useful for this practical session.\n",
        "The whole practical session will be managed with numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxhZd2kb1oBf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "import plotly.graph_objects as go # for visualization\n",
        "from scipy.spatial import KDTree\n",
        "from plyfile import PlyData\n",
        "from scipy.spatial import KDTree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics as sk_metrics\n",
        "\n",
        "global_render_mode = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to the first practical session, we use [plotly](https://plotly.com/python) for point cloud visualization.\n",
        "Note that the function can work with both colors as a 1D array or with a 3D array.\n",
        "\n",
        "In the later case, colors must be intergers in $[0,255]$.\n",
        "\n",
        "In the former case, the colors are displayed using the color scale \"viridis\"."
      ],
      "metadata": {
        "id": "s6NAiV1bjZ_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_cloud_visu(pts, cls=None):\n",
        "\n",
        "  if global_render_mode:\n",
        "\n",
        "    fig = go.Figure(\n",
        "        data=[\n",
        "            go.Scatter3d(\n",
        "                x=pts[:,0], y=pts[:,1], z=pts[:,2],\n",
        "                mode='markers',\n",
        "                marker=dict(size=3,\n",
        "                            color=cls,\n",
        "                            #colorscale='Viridis',\n",
        "                            )\n",
        "            )\n",
        "        ],\n",
        "        layout=dict(\n",
        "            scene=dict(\n",
        "                xaxis=dict(visible=False),\n",
        "                yaxis=dict(visible=False),\n",
        "                zaxis=dict(visible=False),\n",
        "                aspectmode=\"data\", #this string can be 'data', 'cube', 'auto', 'manual'\n",
        "                #a custom aspectratio is defined as follows:\n",
        "                aspectratio=dict(x=1, y=1, z=0.95)\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "TA45FkNjexbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we load the point clouds (one for training, one for validation).\n",
        "The points are in `pts` and `val_pts` and the class labels (intergers) are in `cls` and `val_cls`."
      ],
      "metadata": {
        "id": "gNLNdOqZkatG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x96Tq-jLvqi1"
      },
      "outputs": [],
      "source": [
        "with open(\"Lille1_1_sub_train.ply\", 'rb') as f:\n",
        "  plydata = PlyData.read(f)\n",
        "  pts = np.stack([plydata['vertex'][\"x\"],plydata['vertex'][\"y\"], plydata['vertex'][\"z\"]], axis=1)\n",
        "  cls = plydata['vertex']['scalar_class'].astype(np.int64)\n",
        "\n",
        "with open(\"Lille1_1_sub_val.ply\", 'rb') as f:\n",
        "  plydata = PlyData.read(f)\n",
        "  val_pts = np.stack([plydata['vertex'][\"x\"],plydata['vertex'][\"y\"], plydata['vertex'][\"z\"]], axis=1)\n",
        "  val_cls = plydata['vertex']['scalar_class'].astype(np.int64)\n",
        "\n",
        "class_mapping = [\n",
        "  \"unclassified\",\n",
        "  \"ground\",\n",
        "  \"building\",\n",
        "  \"pole - road sign - traffic light\",\n",
        "  \"bollard - small pole\",\n",
        "  \"trash can\",\n",
        "  \"barrier\",\n",
        "  \"pedestrian\",\n",
        "  \"car\",\n",
        "  \"natural - vegetation\",\n",
        "]\n",
        "\n",
        "color_map = np.array([\n",
        "    [0,0,0],\n",
        "    [255, 0, 255],\n",
        "    [0, 200, 255],\n",
        "    [150, 240, 255],\n",
        "    [180, 30, 80],\n",
        "    [0, 150, 255],\n",
        "    [50, 120, 255],\n",
        "    [30, 30, 255],\n",
        "    [245, 150, 100],\n",
        "    [0, 175, 0],\n",
        "], dtype=np.uint8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Point cloud preparation\n",
        "\n",
        "The point clouds are too dense if we want to compute in a reasonable time (for a practical session) one complex descriptor for each point and train a classifier on top of it.\n",
        "\n",
        "As preliminary, we select a reasonable number of *support points*, the points that will be use to compute the descriptors.\n",
        "\n",
        "**Question**: compute the new point cloud `val_pts_for_descriptor` and `val_cls_for_descriptor` by downsampling by 10 the validation point cloud (it does not need to, just taking one point every 10 is sufficient)."
      ],
      "metadata": {
        "id": "IM04tnKOrA_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decimate_val_points(val_pts, val_cls):\n",
        "  val_pts_for_descriptor = val_pts[::10]\n",
        "  val_cls_for_descriptor = val_cls[::10]\n",
        "  return val_pts_for_descriptor, val_cls_for_descriptor\n",
        "\n",
        "val_pts_for_descriptor, val_cls_for_descriptor = decimate_val_points(val_pts, val_cls)\n",
        "print(f\"Validation point cloud: {val_pts.shape[0]} points\")\n",
        "print(f\"Validation point cloud for descriptor: {val_pts_for_descriptor.shape[0]} points\")\n",
        "point_cloud_visu(val_pts_for_descriptor, color_map[val_cls_for_descriptor])"
      ],
      "metadata": {
        "id": "dUJXbzazu3vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downsampling the training point cloud is a bit more tricky. Indeeed, the classes are very unbalanced in a urban scene: a lot road points, not so much on cars, very little for poles and traffic signs.\n",
        "Thus, training classic machine learning algorithms, like random forests, can be difficult.\n",
        "\n",
        "**Question:** create the support points `pts_for_descriptor` and `cls_for_descriptor` in a more or less balanced fashion following the given algorithm.\n",
        "\n",
        "Parameters: `max_pts_per_class`, is the maximal number of points for a given class, e.g., 1000.\n",
        "\n",
        "- create lists for points and classes\n",
        "- For each class $c$ (for $c$ > 0, we do not keep the unassigned points)\n",
        "  - get the points corresponding to this class\n",
        "  - if the number of points of this class > `max_pts_per_class`:\n",
        "    - randomly select `max_pts_per_class` in these points\n",
        "  - else:\n",
        "    - select all the points\n",
        "  - add the correspoding points to the containers\n",
        "- convert lists to numpy arrays"
      ],
      "metadata": {
        "id": "Qe-qKIZgwsWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6D50ysf0uV1"
      },
      "outputs": [],
      "source": [
        "def decimate_training_points( pts, cls, max_pts_per_class = 1000):\n",
        "  pts_for_descriptor = []\n",
        "  cls_for_descriptor = []\n",
        "  for cls_id in range(1, cls.max()):\n",
        "    mask = (cls == cls_id)\n",
        "    pts_cl = pts[mask]\n",
        "    if pts_cl.shape[0] <= max_pts_per_class:\n",
        "      pts_for_descriptor.append(pts_cl)\n",
        "      cls_for_descriptor.append(cls[mask])\n",
        "    else:\n",
        "      indices = np.random.permutation(pts_cl.shape[0])[:max_pts_per_class]\n",
        "      pts_cl = pts_cl[indices]\n",
        "      pts_for_descriptor.append(pts_cl)\n",
        "      cls_for_descriptor.append(cls[mask][indices])\n",
        "\n",
        "  pts_for_descriptor = np.concatenate(pts_for_descriptor,0)\n",
        "  cls_for_descriptor = np.concatenate(cls_for_descriptor, 0)\n",
        "\n",
        "  return pts_for_descriptor, cls_for_descriptor\n",
        "\n",
        "pts_for_descriptor, cls_for_descriptor = decimate_training_points( pts, cls, max_pts_per_class = 1000)\n",
        "print(f\"Training point cloud: {pts.shape[0]} points\")\n",
        "print(f\"Training point cloud for descriptor: {pts_for_descriptor.shape[0]} points\")\n",
        "point_cloud_visu(pts_for_descriptor, color_map[cls_for_descriptor])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the descriptors\n",
        "\n",
        "In this section, we will create the descriptors associated with each point.\n",
        "As described in *Thomas et al*, the descriptors rely on the covariance matrix computed on a small neighborhood around each support point.\n",
        "\n",
        "**Question:** compute the neighborhoods of each support point from `pts_for_descriptor` (resp. `val_pts_for_descriptor`) in the orginal point cloud `pts` (resp. 'val_pts').\n",
        "\n",
        "*Note:* you can use the [KDTree from scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html)."
      ],
      "metadata": {
        "id": "2x4fTS0f80ob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y02w0Gqt13HP"
      },
      "outputs": [],
      "source": [
        "def compute_radius_neighborhoods(support_points, points, radius=2):\n",
        "  tree = KDTree(points)\n",
        "  indices = tree.query_ball_point(support_points, radius)\n",
        "  return indices\n",
        "\n",
        "indices = compute_radius_neighborhoods(pts_for_descriptor, pts, radius=2)\n",
        "val_indices = compute_radius_neighborhoods(val_pts_for_descriptor, val_pts, radius=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** compute the covaiance matrix, eigen values, and eigen vectors for a list of points. The function returns the eigen values and eigen vectors, sorted in **descending** order.\n",
        "\n",
        "*Note:* you can use the numpy function for [covariance computation](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) as well as the [eigen values/vector extractor](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html).*texte en italique*\n",
        "\n",
        "*Note-2:* the `eigh` value from numpy return the values in **ascending order**."
      ],
      "metadata": {
        "id": "tCUEBg4C-NET"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1KheZDkupTH"
      },
      "outputs": [],
      "source": [
        "def compute_eigen(pts):\n",
        "  # input: point cloud [N 3]\n",
        "  # output: 3x3 matrix covariance matrix\n",
        "\n",
        "  # compute the covariance matrix\n",
        "  if pts.shape[0] > 1:\n",
        "    cov = np.cov(pts.T, rowvar=True)\n",
        "  else:\n",
        "    cov = np.zeros((3,3))\n",
        "\n",
        "  # diagonalize the covariance matrix\n",
        "  eigvals, eigvecs = np.linalg.eigh(cov)\n",
        "\n",
        "  # reverse the order\n",
        "  eigvals = eigvals[[2,1,0]]\n",
        "  eigvecs = eigvecs[:, [2,1,0]]\n",
        "\n",
        "  return eigvals, eigvecs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** compute eigen values and eigen vectors for all the support points (training and validation) by completing the function. The function loop over all the support points and their neighborhood indices and returns the eigen values and eigen vector as numpy arrays."
      ],
      "metadata": {
        "id": "bcfRATbAAhYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_eigen_for_all_support_points(points, indices):\n",
        "  eigvals = []\n",
        "  eigvecs = []\n",
        "  for ids in tqdm.tqdm(indices):\n",
        "    pts_ = points[ids]\n",
        "    eigvals_, eigvecs_ = compute_eigen(pts_)\n",
        "    eigvals.append(eigvals_)\n",
        "    eigvecs.append(eigvecs_)\n",
        "  eigvals = np.stack(eigvals, axis=0)\n",
        "  eigvecs = np.stack(eigvecs, axis=0)\n",
        "\n",
        "  # safeguard (all eigenvalues should >= 0)\n",
        "  eigvals = np.clip(eigvals, a_min=0, a_max=np.inf)\n",
        "  return eigvals, eigvecs\n",
        "\n",
        "eigvals, eigvecs = compute_eigen_for_all_support_points(pts, indices) # indices has the length of the support points\n",
        "val_eigvals, val_eigvecs = compute_eigen_for_all_support_points(val_pts, val_indices) # indices has the length of the support points"
      ],
      "metadata": {
        "id": "UYkjBUHuAtSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXr0yez66Rg2"
      },
      "source": [
        "We now compute the descriptors per se.\n",
        "In this part of the practical session, we only compute a subset of the descriptors presented in *Thomas et al.*.\n",
        "\n",
        "In the following $\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3$ are the eigen values in descending order, and $v_1, v_2, v_3$ their associated unit eigen vector.\n",
        "\n",
        "\n",
        "**Question:** Linearity describe how much the neighborhood fits a line. It is defined by:\n",
        "$$D_\\text{linearity}(P) = 1 - \\frac{\\lambda_2}{\\lambda_1 + \\epsilon}$$\n",
        "Fill the corresponding function.\n",
        "\n",
        "*Note:* $\\lambda_1$ can be $0$, $\\epsilon = 10^{-12}$ is a small constant to prevent undefined values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaCcHJWQ7xC6"
      },
      "outputs": [],
      "source": [
        "def descriptor_linearity(eigvals):\n",
        "  desc =  1 - eigvals[:,1:2] / (eigvals[:,0:1] + 1e-12)\n",
        "  assert(len(desc.shape)==2) # shape should be [N,1]\n",
        "  return desc\n",
        "\n",
        "d_linearity = descriptor_linearity(eigvals)\n",
        "val_d_linearity = descriptor_linearity(val_eigvals)\n",
        "\n",
        "# display for training point cloud\n",
        "point_cloud_visu(pts_for_descriptor, d_linearity[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Planarity describe how much the neighborhood fits a plane. It is defined by:\n",
        "$$D_\\text{planarity}(P) = \\frac{\\lambda_2 - \\lambda_3}{\\lambda_1 + \\epsilon}$$\n",
        "Fill the corresponding function.\n",
        "\n",
        "*Note:* $\\lambda_1$ can be $0$, $\\epsilon = 10^{-12}$ is a small constant to prevent undefined values."
      ],
      "metadata": {
        "id": "jGPzAqX8FDJD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI8Fa_7C0D9y"
      },
      "outputs": [],
      "source": [
        "def descriptor_planarity(eigvals):\n",
        "  desc = (eigvals[:,1:2] - eigvals[:,2:3]) / (eigvals[:,0:1] + 1e-12)\n",
        "  assert(len(desc.shape)==2) # shape should be [N,1]\n",
        "  return desc\n",
        "\n",
        "d_planarity = descriptor_planarity(eigvals)\n",
        "val_d_planarity = descriptor_planarity(val_eigvals)\n",
        "point_cloud_visu(pts_for_descriptor, d_planarity[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** verticality describe how much the neighborhood structure aligns with the vertical direction, i.e., how much the normal is orthogonal to the vertical direction. It is defined by:\n",
        "$$D_\\text{verticality}(P) = 1 -  \\left| <v_3, e_z>\\right|$$\n",
        "Fill the corresponding function."
      ],
      "metadata": {
        "id": "ZrLPbzhpGd1Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iURTv-9r0s5E"
      },
      "outputs": [],
      "source": [
        "def descriptor_verticality(eigvecs):\n",
        "  desc = 1 - np.abs(eigvecs[:,2,2:3])\n",
        "  assert(len(desc.shape)==2)\n",
        "  return desc\n",
        "\n",
        "d_verticality = descriptor_verticality(eigvecs)\n",
        "val_d_verticality = descriptor_verticality(val_eigvecs)\n",
        "point_cloud_visu(pts_for_descriptor, d_verticality[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** sphericity describe how much the neighborhood fits a sphere. It is defined by:\n",
        "$$D_\\text{sphericity}(P) = \\frac{\\lambda_3}{\\lambda_1 + \\epsilon}$$\n",
        "Fill the corresponding function.\n",
        "\n",
        "*Note:* $\\lambda_1$ can be $0$, $\\epsilon = 10^{-12}$ is a small constant to prevent undefined values."
      ],
      "metadata": {
        "id": "SYJv62SdXOVy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6bZO3gg0LUz"
      },
      "outputs": [],
      "source": [
        "def descriptor_sphericity(eigvals):\n",
        "  desc =  eigvals[:,2:3] / (eigvals[:,0:1] + 1e-12)\n",
        "  assert(len(desc.shape)==2)\n",
        "  return desc\n",
        "\n",
        "d_sphericity = descriptor_sphericity(eigvals)\n",
        "val_d_sphericity = descriptor_sphericity(val_eigvals)\n",
        "point_cloud_visu(pts_for_descriptor, d_sphericity[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** create the final descriptor by concatenating all the previous descriptors"
      ],
      "metadata": {
        "id": "wZhtPxSaYAid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "descriptor = np.concatenate([\n",
        "    d_linearity,\n",
        "    d_planarity,\n",
        "    d_sphericity,\n",
        "    d_verticality,\n",
        "], axis=1)\n",
        "val_descriptor = np.concatenate([\n",
        "    val_d_linearity,\n",
        "    val_d_planarity,\n",
        "    val_d_sphericity,\n",
        "    val_d_verticality,\n",
        "], axis=1)"
      ],
      "metadata": {
        "id": "i3VRPyw-X_4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the descriptor, we can train the classifier.\n",
        "\n",
        "**Question:** train a RandomForest ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)) on the training descriptors and predict on the validation descriptors."
      ],
      "metadata": {
        "id": "jxN_lYNmrPQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(descriptor, cls_for_descriptor)\n",
        "val_predictions = classifier.predict(val_descriptor)\n",
        "point_cloud_visu(val_pts_for_descriptor, color_map[val_predictions])"
      ],
      "metadata": {
        "id": "0GQJmFQjeU52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visual prediction is good, but not enough.\n",
        "\n",
        "**Question:** compute the average Intersection over Union (mIoU) and the IoU per class of the predictions. IoU is the [Jaccard score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)\n",
        "\n",
        "*Note:* do not compute the scores including the class 0, which correspond to the unassigned points."
      ],
      "metadata": {
        "id": "UFVs45VbtwIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = val_cls_for_descriptor > 0\n",
        "print(\"mIoU\", sk_metrics.jaccard_score(val_cls_for_descriptor[mask], val_predictions[mask], labels=list(range(10)), average=\"macro\"))\n",
        "iou_per_class = sk_metrics.jaccard_score(val_cls_for_descriptor[mask], val_predictions[mask], labels=list(range(10)), average=None)\n",
        "for i in range(len(class_mapping)):\n",
        "  print(i, class_mapping[i], iou_per_class[i])\n"
      ],
      "metadata": {
        "id": "ydRwLKrZjfQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Going further\n",
        "\n",
        "In the paper [here](https://people.cmm.minesparis.psl.eu/users/marcoteg/cv/publi_pdf/HuguesThomas/2018_3DV_preprint.pdf) several other descriptors derived from the covariance matrix are described and can be implemented.\n",
        "\n",
        "**Question** Implement the missing descriptors from the paper:\n",
        "- eigen entropy\n",
        "- sum of eigen values\n",
        "- omnivariance\n",
        "- change of curvature\n",
        "- absolute moment\n",
        "- vertical moment\n",
        "- number of points"
      ],
      "metadata": {
        "id": "Cce3nTT3DJKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbBL7L0P7TYl"
      },
      "outputs": [],
      "source": [
        "def descriptor_eigen_entropy(eigvals):\n",
        "  desc = - (eigvals * np.log(eigvals+1e-12)).sum(axis=1, keepdims=True)\n",
        "  assert(len(desc.shape)==2)\n",
        "  return desc\n",
        "\n",
        "def descriptor_sum_of_eigen_values(eigvals):\n",
        "  desc = eigvals.sum(axis=-1, keepdims=True)\n",
        "  assert(len(desc.shape)==2)\n",
        "  return desc\n",
        "\n",
        "def descriptor_omnivariance(eigvals):\n",
        "  desc = eigvals.prod(axis=-1, keepdims=True)**(1./3)\n",
        "  assert(len(desc.shape)==2)\n",
        "  return desc\n",
        "\n",
        "def descriptor_change_of_curvature(eigvals):\n",
        "  desc =  eigvals[:,2:3] / (eigvals.sum(axis=-1, keepdims=True) + 1e-12)\n",
        "  assert(len(desc.shape)==2)\n",
        "  return desc\n",
        "\n",
        "def descriptor_absolute_moment(eigvecs, pts_for_descriptor, pts, indices, order=1):\n",
        "\n",
        "  desc = []\n",
        "  for i,ids in enumerate(tqdm.tqdm(indices)):\n",
        "    pts_ = pts[ids] - pts_for_descriptor[i:i+1]  # [N, 3]\n",
        "    eigv = eigvecs[i] # [3,3]\n",
        "    pts_ = np.abs((pts_ @ eigv)**order).mean(axis=0)\n",
        "    desc.append(pts_)\n",
        "\n",
        "  desc = np.stack(desc, axis=0)\n",
        "  assert(len(desc.shape)==2)\n",
        "  return desc\n",
        "\n",
        "def descriptor_vertical_moment(pts_for_descriptor, pts, indices, order=1):\n",
        "\n",
        "  desc = []\n",
        "  for i,ids in enumerate(tqdm.tqdm(indices)):\n",
        "    pts_ = pts[ids] - pts_for_descriptor[i:i+1]  # [N, 3]\n",
        "    pts_ = np.abs(pts_[:,2:3]**order).mean(axis=0)\n",
        "    desc.append(pts_)\n",
        "\n",
        "  desc = np.stack(desc, axis=0)\n",
        "  assert(len(desc.shape)==2)\n",
        "  return desc\n",
        "\n",
        "def descriptor_number_of_points(indices):\n",
        "  desc = []\n",
        "  for i,ids in enumerate(tqdm.tqdm(indices)):\n",
        "    desc.append(len(ids))\n",
        "  desc = np.array(desc, dtype=np.float32)\n",
        "  desc =  desc.reshape(desc.shape[0],1)\n",
        "  assert(len(desc.shape)==2)\n",
        "  return desc\n",
        "\n",
        "d_eigen_entropy = descriptor_eigen_entropy(eigvals)\n",
        "val_d_eigen_entropy = descriptor_eigen_entropy(val_eigvals)\n",
        "point_cloud_visu(pts_for_descriptor, d_eigen_entropy[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** fill the function that compute the complete descriptor"
      ],
      "metadata": {
        "id": "m1oEQ-xsy1hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_descriptor(eigvals, eigvecs, pts_for_descriptor, pts, indices):\n",
        "  desc = np.concatenate([\n",
        "      descriptor_linearity(eigvals),\n",
        "      descriptor_verticality(eigvecs),\n",
        "      descriptor_sphericity(eigvals),\n",
        "      descriptor_planarity(eigvals),\n",
        "      descriptor_change_of_curvature(eigvals),\n",
        "      descriptor_eigen_entropy(eigvals),\n",
        "      descriptor_absolute_moment(eigvecs, pts_for_descriptor, pts, indices, order=1),\n",
        "      descriptor_number_of_points(indices),\n",
        "      descriptor_omnivariance(eigvals),\n",
        "      descriptor_vertical_moment(pts_for_descriptor, pts, indices, order=1),\n",
        "      descriptor_sum_of_eigen_values(eigvals)\n",
        "  ], axis=1)\n",
        "\n",
        "  return desc\n",
        "\n",
        "print(compute_descriptor(eigvals, eigvecs, pts_for_descriptor, pts, indices).shape)\n"
      ],
      "metadata": {
        "id": "lkbwrqbMzIJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another key ingredient is the multiscale descriptor, i.e., concatenating several descriptors for several neighborhood sizes.\n",
        "\n",
        "**Question:** complete the function that compute the multiscale descriptor."
      ],
      "metadata": {
        "id": "KbkOMuLu2Ipn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4TGzYyb-jow"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_multiscale_descriptor(pts_for_descriptor, pts, radius_list):\n",
        "\n",
        "  desc = []\n",
        "  for radius in radius_list:\n",
        "    print(f\"getting descriptor for radius {radius}...\")\n",
        "    indices = compute_radius_neighborhoods(pts_for_descriptor, pts, radius=radius)\n",
        "    eigvals, eigvecs = compute_eigen_for_all_support_points(pts, indices) # indices has the length of the support points\n",
        "    desc.append(compute_descriptor(eigvals, eigvecs, pts_for_descriptor, pts, indices))\n",
        "  desc = np.concatenate(desc, axis=1)\n",
        "\n",
        "  return desc\n",
        "\n",
        "descriptor = compute_multiscale_descriptor(pts_for_descriptor, pts, [1,2,4])\n",
        "val_descriptor = compute_multiscale_descriptor(val_pts_for_descriptor, val_pts, [1,2,4])\n",
        "print(descriptor.shape, cls_for_descriptor.shape)\n",
        "print(val_descriptor.shape, val_cls_for_descriptor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qestion:** same as before, compute the random forest, the predictions..."
      ],
      "metadata": {
        "id": "cuavLNp_8Vo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(descriptor, cls_for_descriptor)\n",
        "val_predictions = classifier.predict(val_descriptor)\n",
        "point_cloud_visu(val_pts_for_descriptor, color_map[val_predictions])"
      ],
      "metadata": {
        "id": "MJcYjd1gNBmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** and the metrics"
      ],
      "metadata": {
        "id": "v5kZ5_4A8tas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = val_cls_for_descriptor > 0\n",
        "print(\"mIoU\", sk_metrics.jaccard_score(val_cls_for_descriptor[mask], val_predictions[mask], labels=list(range(10)), average=\"macro\"))\n",
        "iou_per_class = sk_metrics.jaccard_score(val_cls_for_descriptor[mask], val_predictions[mask], labels=list(range(10)), average=None)\n",
        "for i in range(len(class_mapping)):\n",
        "  print(i, class_mapping[i], iou_per_class[i])"
      ],
      "metadata": {
        "id": "KdTIbJ2P8MEW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}